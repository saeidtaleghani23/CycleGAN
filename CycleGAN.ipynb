{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, LeakyReLU, Activation, Concatenate, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from instancenormalization import InstanceNormalization\n",
    "from tensorflow.keras.layers import Input, UpSampling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# required function in defining Generator and Discriminator models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Down sample fucntion\n",
    "def downsampling(in_layer: tf.Tensor, num_filters: int, kernel_size: int = 4, strides: int = 2) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Downsamples an input tensor using a Conv2D layer, followed by LeakyReLU activation and \n",
    "    InstanceNormalization.\n",
    "\n",
    "    Args:\n",
    "        in_layer (tf.Tensor): Input tensor to be downsampled.\n",
    "        num_filters (int): Number of filters for the Conv2D layer.\n",
    "        kernel_size (int, optional): Size of the convolutional kernel. Defaults to 4.\n",
    "        strides (int, optional): Stride size for the convolution operation. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: The downsampled output tensor after applying convolution, activation, and normalization.\n",
    "    \"\"\"\n",
    "    downsampled = Conv2D(num_filters, kernel_size=kernel_size, strides=strides, padding='same')(in_layer)\n",
    "    downsampled = LeakyReLU(alpha=0.2)(downsampled)\n",
    "    downsampled = InstanceNormalization()(downsampled)\n",
    "    return downsampled\n",
    "\n",
    "# Up sample function\n",
    "def upsampling(in_layer: tf.Tensor, skip_layer: tf.Tensor, num_filters: int, kernel_size: int = 4, strides: int = 1, dropout_rate: float = 0) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Upsamples an input tensor using UpSampling2D and Conv2D layers, with optional dropout and \n",
    "    InstanceNormalization, followed by concatenation with a skip connection.\n",
    "\n",
    "    Args:\n",
    "        in_layer (tf.Tensor): Input tensor to be upsampled.\n",
    "        skip_layer (tf.Tensor): Tensor to concatenate as a skip connection with the upsampled tensor.\n",
    "        num_filters (int): Number of filters for the Conv2D layer.\n",
    "        kernel_size (int, optional): Size of the convolutional kernel. Defaults to 4.\n",
    "        strides (int, optional): Stride size for the convolution operation. Defaults to 1.\n",
    "        dropout_rate (float, optional): Dropout rate (0 means no dropout). Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: The upsampled output tensor after applying convolution, normalization, and concatenation.\n",
    "    \"\"\"\n",
    "    upsampled = UpSampling2D(size=2)(in_layer)\n",
    "    upsampled = Conv2D(num_filters, kernel_size=kernel_size, strides=strides, padding='same', activation='relu')(upsampled)\n",
    "    if dropout_rate:\n",
    "        upsampled = Dropout(dropout_rate)(upsampled)\n",
    "    upsampled = InstanceNormalization()(upsampled)\n",
    "    upsampled = Concatenate()([upsampled, skip_layer])\n",
    "    return upsampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define U-Net shape generative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(img_shape: tuple, in_channels: int = 3, num_filters: int = 32) -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Builds a U-Net style generator model with downsampling and upsampling layers, often used for \n",
    "    image generation tasks.\n",
    "\n",
    "    Args:\n",
    "        img_shape (tuple): Shape of the input image (height, width, channels).\n",
    "        in_channels (int, optional): Number of channels in the output image. Defaults to 3.\n",
    "        num_filters (int, optional): Base number of filters for the downsampling layers. Defaults to 32.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: The generator model built with U-Net architecture.\n",
    "    \"\"\"\n",
    "    # image shape\n",
    "    input_layer = Input(shape=img_shape)\n",
    "    \n",
    "    # downsampling in U-Net model\n",
    "    down_sample_1 = downsampling(in_layer=input_layer, num_filters=num_filters)\n",
    "    down_sample_2 = downsampling(in_layer=down_sample_1, num_filters=2 * num_filters)\n",
    "    down_sample_3 = downsampling(in_layer=down_sample_2, num_filters=4 * num_filters)\n",
    "    bottleneck = downsampling(in_layer=down_sample_3, num_filters=8 * num_filters)\n",
    "    \n",
    "    # upsampling in U-Net model\n",
    "    upsample_1 = upsampling(in_layer=bottleneck, skip_layer=down_sample_3, num_filters=4 * num_filters)\n",
    "    upsample_2 = upsampling(in_layer=upsample_1, skip_layer=down_sample_2, num_filters=2 * num_filters)\n",
    "    upsample_3 = upsampling(in_layer=upsample_2, skip_layer=down_sample_1, num_filters=num_filters)\n",
    "    upsample_4 = UpSampling2D(size=2)(upsample_3)\n",
    "    \n",
    "    # output layer\n",
    "    output_img = Conv2D(in_channels, kernel_size=4, strides=1, padding='same', activation='tanh')(upsample_4)\n",
    "    \n",
    "    # return the generative model\n",
    "    return Model(input_layer, output_img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define the discriminator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disc_block(in_layer: tf.Tensor, num_filters: int, kernel_size: int = 4, instance_normalization: bool = True) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Builds a convolutional block with Conv2D, LeakyReLU activation, and optional InstanceNormalization, \n",
    "    commonly used in discriminator networks.\n",
    "\n",
    "    Args:\n",
    "        in_layer (tf.Tensor): Input tensor for the block.\n",
    "        num_filters (int): Number of filters for the Conv2D layer.\n",
    "        kernel_size (int, optional): Size of the convolutional kernel. Defaults to 4.\n",
    "        instance_normalization (bool, optional): Whether to apply InstanceNormalization. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: The output tensor after applying convolution, activation, and optional normalization.\n",
    "    \"\"\"\n",
    "    disc_layer = Conv2D(num_filters, kernel_size=kernel_size, strides=2, padding='same')(in_layer)\n",
    "    disc_layer = LeakyReLU(alpha=0.2)(disc_layer)\n",
    "    if instance_normalization:\n",
    "        disc_layer = InstanceNormalization()(disc_layer)\n",
    "    return disc_layer\n",
    "\n",
    "def build_discriminator(img_shape: tuple, num_filters: int = 64) -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Builds a discriminator model using multiple convolutional blocks and outputs a single-channel \n",
    "    feature map. The model uses a sequence of downsampling layers with increasing filter sizes.\n",
    "\n",
    "    Args:\n",
    "        img_shape (tuple): Shape of the input image (height, width, channels).\n",
    "        num_filters (int, optional): Base number of filters for the first convolutional block. Defaults to 64.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: The discriminator model built for distinguishing between real and generated images.\n",
    "    \"\"\"\n",
    "    input_layer = Input(shape=img_shape)\n",
    "    \n",
    "    # First block, without instance normalization\n",
    "    disc_block_1 = disc_block(input_layer, num_filters=num_filters, instance_normalization=False)\n",
    "    \n",
    "    # Subsequent blocks with increasing filters\n",
    "    disc_block_2 = disc_block(disc_block_1, num_filters * 2)\n",
    "    disc_block_3 = disc_block(disc_block_2, num_filters * 4)\n",
    "    disc_block_4 = disc_block(disc_block_3, num_filters * 8)\n",
    "    \n",
    "    # Final output layer\n",
    "    disc_output = Conv2D(1, kernel_size=4, strides=1, padding='same')(disc_block_4)\n",
    "    \n",
    "    # Return the discriminator model\n",
    "    return Model(input_layer, disc_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_filter = 32\n",
    "discriminator_filters = 64\n",
    "# image shape\n",
    "image_height = 256\n",
    "image_width = 256\n",
    "# input shape\n",
    "channels = 3\n",
    "input_shape = (image_height, image_width, channels)\n",
    "# loss weights\n",
    "lambda_cycle = 10.0\n",
    "lambda_identity = 0.1 * lambda_cycle\n",
    "# optimizer\n",
    "optimizer = Adam (learning_rate= 0.0002, beta_1= 0.5)\n",
    "\n",
    "patch = int (image_height / 2**4)\n",
    "patch_gan_shape = (patch, patch, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CycleGAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discriminator models \n",
    "disc_A = build_discriminator(img_shape = input_shape, num_filters = discriminator_filters)\n",
    "disc_A.compile(loss = 'mse',\n",
    "optimizer = optimizer,\n",
    "metrics = ['accuracy'])\n",
    "\n",
    "disc_B = build_discriminator(img_shape = input_shape, num_filters = discriminator_filters)\n",
    "disc_B.compile(loss = 'mse',\n",
    "optimizer = optimizer,\n",
    "metrics = ['accuracy'])\n",
    "\n",
    "# generators model \n",
    "gen_AtoB = build_generator(img_shape = input_shape, in_channels = channels, num_filters = generator_filter)\n",
    "gen_BtoA = build_generator(img_shape = input_shape, in_channels = channels, num_filters = generator_filter)\n",
    "\n",
    "#CycleGAN model\n",
    "real_image_A = Input(shape=input_shape)\n",
    "real_image_B = Input(shape=input_shape)\n",
    "# generate fake samples from both generators\n",
    "fake_image_B = gen_AtoB(real_image_A)\n",
    "fake_image_A = gen_BtoA(real_image_B)\n",
    "\n",
    "# *****Reconstruction Loss*****\n",
    "# reconstruct original samples from both generators using fake images \n",
    "reconstruct_A = gen_BtoA(fake_image_B) # it must be similar to real images from domain A\n",
    "reconstruct_B = gen_AtoB(fake_image_A) # it must be similar to real images from domain B\n",
    "\n",
    "# *****Identity Loss*****\n",
    "# generate identity samples\n",
    "identity_A = gen_BtoA(real_image_A) # it must be equal to real image from domain A\n",
    "identity_B = gen_AtoB(real_image_B) # it must be equal to real image from domain B\n",
    "# disable discriminator training\n",
    "disc_A.trainable = False\n",
    "disc_B.trainable = False\n",
    "\n",
    "# *****Adversarial Loss*****\n",
    "# use discriminator to classify real vs fake \n",
    "output_A = disc_A(fake_image_A)\n",
    "output_B = disc_B(fake_image_B)\n",
    "# Combined model trains generators to fool discriminators to fool discriminators\n",
    "cycle_gan = Model(inputs= [real_image_A, real_image_B],\n",
    "            outputs = [output_A, output_B, reconstruct_A, reconstruct_B, identity_A, identity_B])\n",
    "\n",
    "cycle_gan.compile (loss = ['mse', 'mse', 'mae', 'mae', 'mae', 'mae'], # mse  is used for Adversarial losses while mae is used for identity and reconstruction losses\n",
    "             loss_weights = [1, 1, lambda_cycle, lambda_cycle, lambda_identity, lambda_identity], # how losses are combined to get final loss value\n",
    "             optimizer= optimizer # which optimizer is used\n",
    "             ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training CycleGAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainig(gen_AtoB,\n",
    "                gen_BtoA, \n",
    "                disc_A, \n",
    "                disc_B, \n",
    "                cyclegan, \n",
    "                patch_gan_shape, \n",
    "                epochs,\n",
    "                path= '/dataset/{}'.format(dataset_name),\n",
    "                batch_size = 1, \n",
    "                sample_interval = 50):\n",
    "    # Adversarial loss ground truths\n",
    "    real_labels = np.ones((batch_size,) + patch_gan_shape)\n",
    "    fake_labels = np.zeros((batch_size,) + patch_gan_shape)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch={epoch}')\n",
    "        for idx, (imgs_A, imgs_B) in enumerate(batch_generator(path, batch_size, image_res=[image_height, image_width])) :\n",
    "            # generate fake smaples from both generators\n",
    "            fake_B = gen_AtoB.predict(imgs_A)\n",
    "            fake_A = gen_BtoA.predict(imgs_B)\n",
    "            \n",
    "            # Train discriminators\n",
    "            disc_A_loss_real = disc_A.train_on_batch(imgs_A, real_labels)\n",
    "            disc_A_loss_fake = disc_A.train_on_batch(fake_A, fake_labels)\n",
    "            disc_A_loss = 0.5 * np.add(disc_A_loss_real, disc_A_loss_fake)\n",
    "            \n",
    "            disc_B_loss_real = disc_B.train_on_batch(imgs_B, real_labels)\n",
    "            disc_B_loss_fake = disc_B.train_on_batch(imgs_B, fake_labels)\n",
    "            disc_B_loss = 0.5 * np.add(disc_B_loss_real, disc_B_loss_fake)\n",
    "            # total discriminator loss\n",
    "            discriminator_loss = 0.5 * np.add(disc_A_loss, disc_B_loss)\n",
    "            \n",
    "            # Train generator\n",
    "            gen_loss = cycle_gan.train_on_batch([imgs_A, imgs_B],\n",
    "                                                [\n",
    "                                                 real_labels, real_labels, \n",
    "                                                 imgs_A, imgs_B,\n",
    "                                                 imgs_A, imgs_B\n",
    "                                                 ]\n",
    "                                                )\n",
    "            # training updates every 50 iterations\n",
    "            if idx % 50 == 0:\n",
    "                print(f'[Epoch {idx}/{epoch}] '\n",
    "                        f'[Discriminator loss: {discriminator_loss[0]} Accuracy: {100 * discriminator_loss[1]:.2f}] '\n",
    "                        f'[Adversarial loss (A to B): {gen_loss[0]}] '\n",
    "                        f'[Adversarial loss (B to A): {gen_loss[1]}] '\n",
    "                        f'[Reconstruction loss (A): {gen_loss[2]}] '\n",
    "                        f'[Reconstruction loss (B): {gen_loss[3]}] '\n",
    "                        f'[Identity loss (A): {gen_loss[4]}] '\n",
    "                        f'[Identity loss (B): {gen_loss[5]}]')\n",
    "            \n",
    "            # plot and save progress every few iterations\n",
    "            if idx % sample_interval == 0:\n",
    "                plot_sample_images(gen_AtoB, \n",
    "                                   gen_BtoA,\n",
    "                                   path=path,\n",
    "                                   epoch = epoch,\n",
    "                                   batch_num= idx,\n",
    "                                   output_dir= 'images')\n",
    "                            \n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
